{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-F6HoE9Yjdnl"
      },
      "source": [
        "# 图书推荐教程\n",
        "### 数据悦读大赛作品\n",
        "这个笔记本展示了如何对原始数据集进行预处理、特征提取、模型训练、评估等，最终将生成的数据传送至远端服务器上进行推荐。笔记本中可能需要修改的是原始数据的存放路径，即文件中的DataDir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U_YPAK5Ujdnp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ProjectFolderDir = './'   # 项目文件夹的路径是相对于本笔记本路径而言的，当然也可以使用绝对路径\n",
        "DataDir = './data/raw/' # 原始数据路径，根据实际情况修改"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rdsSxmcjdnq"
      },
      "source": [
        "初步分析阶段先将字段以字符串形式录入，便于后期统一格式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TLh-XcUJjdnr"
      },
      "outputs": [],
      "source": [
        "BigData = pd.read_csv(DataDir + \"浙江大学-图书外借数据-2013~2018.csv\", encoding='utf8', dtype=str)       \n",
        "# 注意~是英文输入，读取不到可以重命名一下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wU1YolFejdns"
      },
      "outputs": [],
      "source": [
        "SmallData = pd.read_csv(DataDir + \"浙江大学-图书外借数据-2019.csv\", encoding='utf8', dtype=str)         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpgm-qUQjdnt"
      },
      "source": [
        "下面是构建模型所需要的数据列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xJzZpvmLjdnt"
      },
      "outputs": [],
      "source": [
        "user_columns = ['PATRON_ID', 'STUDENT_GRADE', 'PATRON_DEPT', 'PATRON_TYPE']\n",
        "# 用户侧特征       用户id          年级            学生学院       学生类型\n",
        "item_columns = ['ITEM_ID', 'SUBLIBRARY', 'ITEM_CALLNO', 'PUBLISH_YEAR', 'AUTHOR', 'TITLE', 'PRESS']\n",
        "# 物品侧特征       记录号      馆藏地         图书索书号        出版年          作者       题目    出版社\n",
        "time = 'LOAN_DATE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY1n867jjdnu"
      },
      "source": [
        "将两份数据集合并"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_EfMIkpDjdnu"
      },
      "outputs": [],
      "source": [
        "UsedColumns = user_columns + item_columns + [time]\n",
        "UsedData = pd.concat([BigData[UsedColumns], SmallData[UsedColumns]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHEduvnwjdnv"
      },
      "source": [
        "本项目将借阅记录中的空值字段采用相同值’na'进行填充，特征处理时将其作为空特征。对于图书类别，将其索书号按/分割后得到图书大类CALLNO1与小类CALLNO2，将借阅日期转为整形，便于之后比较大小。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AtPc6Mv2jdnv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_Bletter(str0):   # 取出大写字母\n",
        "    b = re.sub(u\"([^\\u0041-\\u007a])\", \"\", str0)\n",
        "    return b\n",
        "\n",
        "UsedData = UsedData.fillna(value='na')\n",
        "UsedData['CALLNO1'] = UsedData['ITEM_CALLNO'].str.split('/', expand=True)[0].map(lambda x: get_Bletter(str(x)))\n",
        "UsedData['CALLNO2'] = UsedData['ITEM_CALLNO'].str.split('/', expand=True)[1].map(lambda x: get_Bletter(str(x)))\n",
        "UsedData[time]=UsedData[time].astype(int)\n",
        "UsedData = UsedData.drop(columns='ITEM_CALLNO')  # 删掉不再需要的列"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXXIdeYPjdnv"
      },
      "source": [
        "此时可以看到UsedData中的空值消失，并且多出了两个类别列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGtRpGvyjdnw",
        "outputId": "ba730091-a207-415f-d749-c76eb7f1b4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2260649 entries, 0 to 192700\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Dtype \n",
            "---  ------         ----- \n",
            " 0   PATRON_ID      object\n",
            " 1   STUDENT_GRADE  object\n",
            " 2   PATRON_DEPT    object\n",
            " 3   PATRON_TYPE    object\n",
            " 4   ITEM_ID        object\n",
            " 5   SUBLIBRARY     object\n",
            " 6   PUBLISH_YEAR   object\n",
            " 7   AUTHOR         object\n",
            " 8   TITLE          object\n",
            " 9   PRESS          object\n",
            " 10  LOAN_DATE      int64 \n",
            " 11  CALLNO1        object\n",
            " 12  CALLNO2        object\n",
            "dtypes: int64(1), object(12)\n",
            "memory usage: 241.5+ MB\n"
          ]
        }
      ],
      "source": [
        "UsedData.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoaCMmyjdnw"
      },
      "source": [
        "将数据存入./data/processed中，作为后续分析数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-GURml7jdnw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "if not os.path.exists(ProjectFolderDir+\"data/processed/\"):\n",
        "    os.makedirs(ProjectFolderDir+\"data/processed/\") \n",
        "\n",
        "UsedData.to_csv(ProjectFolderDir+\"data/processed/ZJULibrary2013_2019.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FnWxKoEkRPXZ"
      },
      "outputs": [],
      "source": [
        "del BigData # 将之前的大数据变量清空，空出一部分内存\n",
        "del SmallData\n",
        "del UsedData"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y4GK7PA-4JvE"
      },
      "source": [
        "#### 若已将两个数据集合并可以直接从此处开始\n",
        "调用代码库中的主函数进行模型训练、评估等操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RqzHKeH54If7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "sys.path.append(ProjectFolderDir)\n",
        "\n",
        "from scripts.MainFile import main\n",
        "\n",
        "# 创建缓存目录\n",
        "\n",
        "if not os.path.exists(ProjectFolderDir+\"temp/\"):\n",
        "    os.makedirs(ProjectFolderDir+\"temp/\")\n",
        "if not os.path.exists(ProjectFolderDir+\"log/\"):\n",
        "    os.makedirs(ProjectFolderDir+\"log/\")\n",
        "\n",
        "# 这边可能会报warning:Please update jupyter and ipywidgets，不过对本项目无影响"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yq7jM8rG62i"
      },
      "source": [
        "调用主函数即可，运行的日志文件保存在ProjectFolderDir/log目录下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "moinQ8105-oQ",
        "outputId": "305bd416-826e-4de4-e2ea-ff93fd595f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start: 2023-07-25 20:17:08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generate train set, validation set and test set:: 100%|██████████| 118358/118358 [00:28<00:00, 4179.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_train: 6246232, n_val: 1561559, n_test: 867533\n",
            "40557 cold start user droped \n",
            "train set, validation set and test set have saved in data/processed/data_process.npy\n",
            "standard data has been generated\n",
            "epoch: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 6100/6100 [22:35<00:00,  4.50it/s, loss=0.492]\n",
            "calculate auc on train data: 100%|██████████| 6100/6100 [01:14<00:00, 81.34it/s]\n",
            "calculate auc on validation data: 100%|██████████| 1525/1525 [00:19<00:00, 78.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc on train data: 0.8632906171460298\n",
            "auc on validation data: 0.8356946873747348\n",
            "epoch: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 6100/6100 [26:20<00:00,  3.86it/s, loss=0.385]\n",
            "calculate auc on train data: 100%|██████████| 6100/6100 [01:14<00:00, 81.49it/s]\n",
            "calculate auc on validation data: 100%|██████████| 1525/1525 [00:19<00:00, 78.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc on train data: 0.9176696335272725\n",
            "auc on validation data: 0.8873061444951892\n",
            "epoch: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 6100/6100 [26:22<00:00,  3.85it/s, loss=0.353]\n",
            "calculate auc on train data: 100%|██████████| 6100/6100 [01:15<00:00, 81.33it/s]\n",
            "calculate auc on validation data: 100%|██████████| 1525/1525 [00:19<00:00, 79.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc on train data: 0.9422463163029229\n",
            "auc on validation data: 0.9065038301786308\n",
            "epoch: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 6100/6100 [26:21<00:00,  3.86it/s, loss=0.326]\n",
            "calculate auc on train data: 100%|██████████| 6100/6100 [01:13<00:00, 82.97it/s]\n",
            "calculate auc on validation data: 100%|██████████| 1525/1525 [00:19<00:00, 79.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc on train data: 0.9591565473798918\n",
            "auc on validation data: 0.9153590987479483\n",
            "epoch: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: 100%|██████████| 6100/6100 [26:19<00:00,  3.86it/s, loss=0.303]\n",
            "calculate auc on train data: 100%|██████████| 6100/6100 [01:13<00:00, 82.75it/s]\n",
            "calculate auc on validation data: 100%|██████████| 1525/1525 [00:19<00:00, 78.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc on train data: 0.9709604528656165\n",
            "auc on validation data: 0.9181194249572948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "calculate confusion matrix on test data: 100%|██████████| 848/848 [00:11<00:00, 75.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "confusion_matrix on test data: {'tn': 0.6883668978586406, 'fp': 0.06202415354804947, 'fn': 0.07842814048572215, 'tp': 0.17118080810758785}\n",
            "auc: 0.8595, recall: 0.6858, precision: 0.7340\n",
            "generated user embedding and item embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "user inference: 100%|██████████| 76/76 [00:01<00:00, 53.81it/s]\n",
            "item inference: 100%|██████████| 685/685 [00:05<00:00, 126.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'HR': 0.24606367527409673, 'n_hit': 19144, 'n_total': 77801}\n",
            "embedding has been saved to ./temp/\n",
            "Finish: 2023-07-25 22:38:29\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main(ProjectFolderDir,  # 之前定义的项目路径\n",
        "      neg_ratio= 3,  # 负采样倍率\n",
        "      min_item= 5,  # 最短序列要求\n",
        "      seq_max_len= 20,  # 截断序列长度\n",
        "      load= False, # 是否从已有数据读取\n",
        "      batch_size= 1024, # batch大小\n",
        "      user_params= [512, 512, 256, 128, 64], # 读者塔MLP参数\n",
        "      item_params= [1024, 512, 256, 128, 64], # 图书塔MLP参数，要确保最后的维度一致\n",
        "      temperature= 0.02, # 温度系数\n",
        "      learning_rate= 1e-4, # 学习率\n",
        "      weight_decay= 1e-4, # 正则化系数\n",
        "      optimizer_fn= torch.optim.Adam,  # 优化器 \n",
        "      epoch= 5, # 训练epoch\n",
        "      topk= 100 #推荐topk个商品 \n",
        "    )  # 大约需要1小时，Hit Rate 25%  最后的评估可能会耗时较久"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "50Wpeh4WHOAw"
      },
      "source": [
        "运行结束后，在项目路径{ProjectFolderDir}下会产生文件\n",
        "\n",
        "{ProjectFolderDir}/data/processed/ZJULibrary2013_2019.csv # 合并整理后的数据集\n",
        "\n",
        "{ProjectFolderDir}/data/processed/data_process.npy # 训练集，验证集，测试集  \n",
        "\n",
        "{ProjectFolderDir}/data/processed/item_user.npy # 用于召回时匹配embedding  *\n",
        "\n",
        "{ProjectFolderDir}/data/processed/raw_id_maps.npy # 原始的id字典    *\n",
        "\n",
        "{ProjectFolderDir}/log/{Start time}.txt # 以开始训练时间命名的日志文件\n",
        "\n",
        "{ProjectFolderDir}/temp/item.ann.index # 保存的用于召回的索引文件  *\n",
        "\n",
        "{ProjectFolderDir}/temp/item_embeddding.pth # 图书的embedding向量  *\n",
        "\n",
        "{ProjectFolderDir}/temp/user_embedding.pth # 读者的embedding向量  *\n",
        "\n",
        "{ProjectFolderDir}/temp/model.pth # 训练的模型参数\n",
        "\n",
        "其中注释后标*的是召回需要的文件（需要上传至服务器）"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "可以在终端中运行\n",
        "\n",
        "python ./scripts/RecItem.py '997e765063b98413f5b079c026468f8'\n",
        "\n",
        "测试推荐"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "torchrec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
